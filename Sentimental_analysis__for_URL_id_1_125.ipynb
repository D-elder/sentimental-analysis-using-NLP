{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_toHNa_C5ZB",
    "outputId": "906f35ee-acaa-44ed-f8ff-61ee835e17c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12432851239669421, subjectivity=0.46138647607803435)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9918, 'neg': 0.041, 'neu': 0.88, 'pos': 0.079}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import contractions\n",
    "import textblob as TextBlob\n",
    "from textblob import Word, TextBlob\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Loading data. \n",
    "with open('URL_id_1.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOK8261dpSBg",
    "outputId": "1b771bc1-0c27-4bf8-9898-98a94fc9bef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XF3Q3W0EEMeA",
    "outputId": "bc1cc791-c3ff-4bd2-f0b5-464d0c6b45cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.13939249639249635, subjectivity=0.48743530543530517)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9991, 'neg': 0.04, 'neu': 0.81, 'pos': 0.15}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_2.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zJHif8EwDExR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM2KrqyGF10E",
    "outputId": "42ab145d-24a4-4e8d-8350-e3dec2410b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12681194511702987, subjectivity=0.5424608371854138)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.028, 'neu': 0.81, 'pos': 0.162}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_3.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jzEb6s4DE1X",
    "outputId": "a7b30c3b-6c80-429c-db8e-2eb4b57024fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6WDkMIiADE9_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acanyFdkF282",
    "outputId": "8f58829c-6262-47d8-e157-e333de63376f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06277205867049618, subjectivity=0.45687307484182443)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9989, 'neg': 0.0, 'neu': 0.86, 'pos': 0.14}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_4.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "  \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OgYBfN4fDFAf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxmUQnpmF5bq",
    "outputId": "c878ec37-ee0f-438a-f883-dfce0ef2933a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.014657903072537216, subjectivity=0.4827054551139917)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.029, 'neu': 0.823, 'pos': 0.148}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_5.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JI4x-OwnDFDp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djoJLF7NF6yJ",
    "outputId": "b69da3f8-9fd5-496f-ef70-4eeb961cc2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.13991024266253618, subjectivity=0.5278390060500158)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.999, 'neg': 0.017, 'neu': 0.886, 'pos': 0.097}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_6.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bBWooCgtDFGP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqOrt0_nF7zJ",
    "outputId": "6c8d4c38-803e-4869-b911-dacc04901c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07347115536902124, subjectivity=0.5275822246858834)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.052, 'neu': 0.785, 'pos': 0.163}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_7.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "drHD0mTJDFJK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJqnN6jFF8_E",
    "outputId": "0ca395c3-8f99-4116-c194-ae1c67ed5701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08644426590683783, subjectivity=0.3578602779178588)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.025, 'neu': 0.814, 'pos': 0.161}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_8.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RAGTwxmyDFL1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzVFd0KlF953",
    "outputId": "e6ff3084-3f14-49a1-eed6-97790a0ba280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07743037518037515, subjectivity=0.35631818181818076)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 1.0, 'neg': 0.024, 'neu': 0.772, 'pos': 0.203}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_9.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-WzoHK8dDFOq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8uZ4y7iF_RW",
    "outputId": "ad1f6585-6ed5-496c-ff1b-851d8cde0981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.049203821656050944, subjectivity=0.46544585987261167)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9984, 'neg': 0.181, 'neu': 0.671, 'pos': 0.148}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_10.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gAbJs9I0DFT3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7w-0P6lGAWp",
    "outputId": "3df3becd-bafa-4099-c330-58d47ecf39b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.013275613275613277, subjectivity=0.4986471861471857)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.018, 'neu': 0.798, 'pos': 0.183}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_11.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MNAKOooxDFXK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikQcQMJ5GBoT",
    "outputId": "10a71365-22ba-4382-a041-9257b2d5c087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.16448113439590717, subjectivity=0.46342557064147966)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9993, 'neg': 0.048, 'neu': 0.823, 'pos': 0.129}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_12.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vzte3F81DFZl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqimlZ6lGDM6",
    "outputId": "b7c8d20e-ce3b-45da-dcb6-0dbff8023d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07543414798458026, subjectivity=0.3458344249410527)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.051, 'neu': 0.77, 'pos': 0.179}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_13.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Lcetf1qIDFcU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rr4JkitGEF3",
    "outputId": "ff2c9526-3c95-4be3-f98b-d4e4274a1db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.04562803608605136, subjectivity=0.4087532696311322)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9992, 'neg': 0.031, 'neu': 0.801, 'pos': 0.168}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_14.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hET5hFFFDFgN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LnptjIOGFF2",
    "outputId": "6d763d92-bb74-4abd-955b-85032c758a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09308236715965422, subjectivity=0.46292081735299406)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9803, 'neg': 0.14, 'neu': 0.717, 'pos': 0.143}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_15.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "97FrVfMRDFkA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDTiH1zUGGDV",
    "outputId": "8b84bc94-ac4e-4eac-9182-c3be21c24542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11727244977244969, subjectivity=0.4703760128760126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.015, 'neu': 0.842, 'pos': 0.143}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_16.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AUro9hPFDFmt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzOGPm56GG-D",
    "outputId": "7cb27e91-c095-45d8-94f1-0253d0568e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11989253343467467, subjectivity=0.45554626466927184)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.051, 'neu': 0.798, 'pos': 0.151}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_17.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5CBHC9KrDFqR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUMrvqTlGH7o",
    "outputId": "dd3553f2-84c2-417a-df30-ced08c8a8f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05841715799537883, subjectivity=0.4135341141476108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.085, 'neu': 0.722, 'pos': 0.193}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_18.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eD0mC2JEDFxn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrmvMt74GI04",
    "outputId": "ab4f6ed3-321c-401a-e723-03076cbf740b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08315662337662333, subjectivity=0.472151601731602)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.073, 'neu': 0.792, 'pos': 0.135}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_19.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "joyIlCJlDF0g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4vCKHyEGJu_",
    "outputId": "bc35a211-107a-4633-e21f-2772401eaad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12234347837202171, subjectivity=0.4824003178579947)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.044, 'neu': 0.797, 'pos': 0.159}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_20.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_pHOpWalHELc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3fFltV9HOig",
    "outputId": "9ea224cf-f10a-4830-b6d5-960cabb0f1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.026654580734977236, subjectivity=0.4962718137222544)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.049, 'neu': 0.797, 'pos': 0.154}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_21.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ayk3OKMDHEWp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QY2RLIm4HQDn",
    "outputId": "b2d695de-0051-4cf4-b7d3-a47093ad1d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10414437381866959, subjectivity=0.4332695033839398)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.042, 'neu': 0.797, 'pos': 0.16}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_22.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "sc5-dQwrHEaE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd7nkZmpHRSg",
    "outputId": "60ac8cf2-893b-402e-9d71-782becfb06de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.1035135186450976, subjectivity=0.37713602187286416)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9986, 'neg': 0.029, 'neu': 0.854, 'pos': 0.117}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_23.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lq8GNVxbHEdh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCLzOMDjHSqC",
    "outputId": "b3f45f80-ffed-432c-9c3a-21fcf135c82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10301999587713878, subjectivity=0.5095728912055448)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.053, 'neu': 0.812, 'pos': 0.135}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_24.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    " \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Uuwgg5CtHEgF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D61EvkOHT14",
    "outputId": "3e9b27c4-efd2-4293-8e24-b8d909a2045e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.14455755029284437, subjectivity=0.555544308632544)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9992, 'neg': 0.058, 'neu': 0.776, 'pos': 0.165}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_25.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ffgxZxQgHEiq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jctbyKJeHVBN",
    "outputId": "3a7421da-bf58-42f0-d95c-3f26509b53e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.15707900432900435, subjectivity=0.5189260461760465)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.045, 'neu': 0.796, 'pos': 0.158}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_26.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    " \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jK49R1GzHEls"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqP265llHWS1",
    "outputId": "f813b5d3-586c-47c2-f413-e57613052e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11294875199150287, subjectivity=0.4313398590257326)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9995, 'neg': 0.052, 'neu': 0.811, 'pos': 0.137}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_27.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZC1Bx-S8HEoR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfRPQMNZHXka",
    "outputId": "0f841e7a-21ea-45b9-b8a4-80ea427872d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11294875199150287, subjectivity=0.4313398590257326)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9995, 'neg': 0.052, 'neu': 0.811, 'pos': 0.137}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_28.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ikUjDppaHEq2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS5lEAi0HYzB",
    "outputId": "88266ac5-08ec-4e08-8483-f3cec227053d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07731783824543007, subjectivity=0.35824612127531835)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.048, 'neu': 0.814, 'pos': 0.137}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_29.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zRNiffTGHEty"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBA2FCexHazS",
    "outputId": "2573a864-23c4-4f70-ad5d-7b908cef9728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12245608610270259, subjectivity=0.3745599930562338)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.031, 'neu': 0.818, 'pos': 0.151}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_30.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "046Wjw6_HEwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfmzKwtwHb4H",
    "outputId": "e12d8d4d-7a9b-4e8b-c8aa-c65b1198fb16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.03526401892680963, subjectivity=0.46314611018826096)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.028, 'neu': 0.797, 'pos': 0.175}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_31.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6y1tF8V1HEzC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbjPy9xZHc-L",
    "outputId": "d832c2c0-7af3-4196-87d4-222b0ea0546c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.13494477485131695, subjectivity=0.5374638912489377)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9993, 'neg': 0.005, 'neu': 0.843, 'pos': 0.151}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_32.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yXLTf_u0HE12"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPRdR_F3HeVa",
    "outputId": "d27af0cb-5311-4d29-cd73-9043809f6b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11538426433004739, subjectivity=0.5107640518785097)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.071, 'neu': 0.716, 'pos': 0.213}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_33.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "BXk9SO75HE4m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGQyDJYmHfbl",
    "outputId": "3c318958-d177-43f8-cee4-5d3891a8f176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.18165524842130348, subjectivity=0.43875908495174537)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.008, 'neu': 0.813, 'pos': 0.179}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_34.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1y4YIRaTHE9r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alU3NX7wHgg5",
    "outputId": "ae01250e-03c7-4afa-fa3f-572fff2c95b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.0673930364114413, subjectivity=0.32803959843837144)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9993, 'neg': 0.017, 'neu': 0.864, 'pos': 0.119}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_35.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PNBLBGx7HFCo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2BjmXawHhmw",
    "outputId": "86b77b4a-8c9f-4224-c139-7c2bb25adbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10544143356643355, subjectivity=0.3781007187257187)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9934, 'neg': 0.052, 'neu': 0.83, 'pos': 0.118}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_36.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "bsxoKdT-HFGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LboVw0wgHinU",
    "outputId": "554509c0-b887-42ec-9872-f43a213a9e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.1325089000911369, subjectivity=0.4895388712165027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.097, 'neu': 0.69, 'pos': 0.213}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_37.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Vc2vL7_8HFKo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VmyJ7HIHkGQ",
    "outputId": "d0c06752-4285-4f9e-a3ec-5140b640a232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.014457570207570195, subjectivity=0.3667125950972107)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9991, 'neg': 0.129, 'neu': 0.779, 'pos': 0.092}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_38.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "VdNRDPZpHFMt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAAj4fD_HlOd",
    "outputId": "9dc28cfe-ecf1-45f1-f1dd-f2df0b1f49d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.1503409090909091, subjectivity=0.40179717341482046)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.997, 'neg': 0.025, 'neu': 0.886, 'pos': 0.089}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_39.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iNenBMCpHFOu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXDQZYBZHmih",
    "outputId": "108a51d3-a8e9-4df7-b13a-9cea3d1899e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.20156091885460833, subjectivity=0.4499825417301155)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.066, 'neu': 0.748, 'pos': 0.186}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_40.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "XfqERGyXHFQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR9LgHaMHnom",
    "outputId": "33fbf3ba-c219-4aa2-8673-af7dfc21ab1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.26986660929432016, subjectivity=0.5104618473895584)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.005, 'neu': 0.819, 'pos': 0.177}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_41.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "XPeKQOKxHFSl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_TyRwvXHo4W",
    "outputId": "b02e6f75-8d8f-4672-94d9-b388ab4258c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.16984126984126988, subjectivity=0.4073908730158729)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.0, 'neu': 0.77, 'pos': 0.23}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_42.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ag0pBvZ9HFU_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6VRZY0mH7Ki",
    "outputId": "36da15cb-8743-413c-d40a-702d4863f5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.14139407467532467, subjectivity=0.4479677353896105)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.023, 'neu': 0.837, 'pos': 0.14}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_43.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgVqpdEUH8Yd",
    "outputId": "6878f890-4f24-463e-80d3-3c21e7e5ef9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12013891172119018, subjectivity=0.35421379253657725)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.021, 'neu': 0.782, 'pos': 0.197}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_44.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "IFOBNEBVHFXC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAiGFPo4H9eP",
    "outputId": "307bbdb5-5f4a-4afd-fa3c-03061d3e6359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.24140109890109887, subjectivity=0.5186263736263736)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9994, 'neg': 0.036, 'neu': 0.732, 'pos': 0.233}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_45.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ZkuovBmxHFY-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBMAw3M0H-3H",
    "outputId": "f4465466-34a0-4a7e-c289-2a1bed674b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.03928571428571429, subjectivity=0.2640269151138716)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9887, 'neg': 0.0, 'neu': 0.885, 'pos': 0.115}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_46.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "i5LIKVbqHFbw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuBrZjbeIAFs",
    "outputId": "4b502316-5efb-47dd-fb2e-86cb1b162e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.034931547619047626, subjectivity=0.46554166666666696)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9923, 'neg': 0.171, 'neu': 0.673, 'pos': 0.156}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_47.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "WhPKOY7IHFff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cg61X5pqIBpX",
    "outputId": "d98c1a5d-39fa-4c51-a38d-c689aa25da91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.38249999999999995, subjectivity=0.4674999999999999)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9992, 'neg': 0.043, 'neu': 0.651, 'pos': 0.307}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_48.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EmzfSe53HFiM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z54kpr6QIC1R",
    "outputId": "4bfe38e8-df61-4f9a-ff27-37d139c0fbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08759256198347104, subjectivity=0.37596804407713486)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.045, 'neu': 0.797, 'pos': 0.158}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_49.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    " \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "mnBZevUMHFmg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfJwxgYxID4K",
    "outputId": "af0849c4-daec-4c5e-f447-aefc13fc3b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.03949327458256029, subjectivity=0.37997333024118746)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9987, 'neg': 0.203, 'neu': 0.685, 'pos': 0.112}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_50.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    " \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "BaNC6HvxHFpG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJcrB5slIFk-",
    "outputId": "ccc21639-3c59-4d72-acfa-2c21bd159e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05530946530946528, subjectivity=0.4934854334854331)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9816, 'neg': 0.105, 'neu': 0.797, 'pos': 0.099}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_51.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "0wW0HkaIHFri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nsqVe0sIG6Y",
    "outputId": "40aa3297-a83a-4107-9613-3fbbd7ea3535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07994843797391564, subjectivity=0.37233393994540487)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9852, 'neg': 0.066, 'neu': 0.849, 'pos': 0.085}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_52.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "3q2jZcSKHFxK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM4sRGQlIIAZ",
    "outputId": "9b728073-bc5b-44cc-c2b6-18c41a4ca485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.12915013147771764, subjectivity=0.499098090238674)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.042, 'neu': 0.808, 'pos': 0.149}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_53.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5GHHPY6MHFzV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jHv36W1IJRc",
    "outputId": "e488cf52-56f1-4df9-8361-fd102fc4bb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.03833435083435082, subjectivity=0.3835113960113959)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9993, 'neg': 0.043, 'neu': 0.85, 'pos': 0.107}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_54.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9svpKkTVHF1n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OI0XTXuIKiZ",
    "outputId": "24885243-f559-45fc-ce57-011b84d44d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.19309668989547038, subjectivity=0.45604563119807034)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.026, 'neu': 0.783, 'pos': 0.191}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_55.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "z0_5R-xiHF6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBtZpSrJILlY",
    "outputId": "84ce963f-5c37-46c6-de90-5c7cd1d1319f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.21586222355050802, subjectivity=0.43622484558676994)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.018, 'neu': 0.833, 'pos': 0.149}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_56.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "zP2RqtqFHF9i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuUrUR2IIM17",
    "outputId": "22714e3d-49bd-4347-f457-dbf0a7a84f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.25092449922958404, subjectivity=0.4854363856482502)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.043, 'neu': 0.656, 'pos': 0.301}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_57.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "IzlhJC8THGAp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNjLeUlSIN7W",
    "outputId": "1baf6c6c-6c90-4bd9-9859-eef51339f399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08463035645788806, subjectivity=0.4535507793893874)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.025, 'neu': 0.832, 'pos': 0.143}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_58.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    " \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "lDyNAvGYHGC9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjbqtrsUIPFQ",
    "outputId": "cdb8a6d1-7e0f-49a2-9b4a-3761b334dd63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05675351288056206, subjectivity=0.5910119047619051)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9959, 'neg': 0.2, 'neu': 0.616, 'pos': 0.183}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_59.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "-1ngV4fDHGFk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reEvpQFUIQE-",
    "outputId": "3a594f09-9daf-4c81-a7e0-f9fee622bde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.32571589446589444, subjectivity=0.6066441441441444)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.064, 'neu': 0.754, 'pos': 0.182}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_60.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "orQEvhz9HGID"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyicUXomIRoO",
    "outputId": "3cea9849-da72-4de3-824f-2a279351b8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09670129870129868, subjectivity=0.4229032456904795)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9902, 'neg': 0.132, 'neu': 0.715, 'pos': 0.153}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_61.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "90AtrlnJHGK9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rpxi_NZsISvI",
    "outputId": "92ed18e0-9336-4010-dd03-7da6198e4ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07024269167126307, subjectivity=0.3150875829447258)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.048, 'neu': 0.83, 'pos': 0.122}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_62.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "U5XrpdDlHGNu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES5egLSeITrj",
    "outputId": "602c9138-a0e6-4f85-eb69-0027ca53caaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05418673087212417, subjectivity=0.5739209916176212)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9973, 'neg': 0.202, 'neu': 0.618, 'pos': 0.179}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_63.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "o4G3JcMtHGQP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-ieNUJtIU0p",
    "outputId": "3703ae46-9ba8-42f1-869c-279f96c43fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.028313943463943465, subjectivity=0.4495981351981356)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9994, 'neg': 0.104, 'neu': 0.741, 'pos': 0.155}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_64.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "S5rAjEEoHGTq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKVGt9BzIWNm",
    "outputId": "66aee86e-a1d7-4848-df63-301bf01e66b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09162201434928707, subjectivity=0.40528055860223694)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.999, 'neg': 0.059, 'neu': 0.789, 'pos': 0.152}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_65.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "CQxbhzUcHGWS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lW4lZFJIXmx",
    "outputId": "e6a9e3bc-d3a0-46d7-e510-664832e32f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.0700530652143556, subjectivity=0.36551781551781554)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9995, 'neg': 0.061, 'neu': 0.79, 'pos': 0.149}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_66.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "t7B-OHWJHGcE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8C_eE175IYxi",
    "outputId": "f7f2c3e6-76b6-4a4a-942d-f3c32a6faa4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.035141137401411386, subjectivity=0.38769928701435563)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9817, 'neg': 0.094, 'neu': 0.831, 'pos': 0.074}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_67.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "8HzzG1OkHGfT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brPAF5H4IaCp",
    "outputId": "b9204fb9-ded0-4e6c-dd19-09245c576dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05011369292728178, subjectivity=0.327881004013408)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.087, 'neu': 0.808, 'pos': 0.105}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_68.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "58U7z4xkHGh-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2C9NH-XIbTz",
    "outputId": "e6175ed3-c81a-4288-dce7-c11d83d1b00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.013643074802292678, subjectivity=0.442835921062178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.056, 'neu': 0.822, 'pos': 0.121}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_69.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "_GmRSQlMHGko"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eryfdZxUIcnN",
    "outputId": "46b104af-95c6-46ea-dc4c-8a868410d460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.01077244276342216, subjectivity=0.35110707591377716)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.8909, 'neg': 0.114, 'neu': 0.766, 'pos': 0.119}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_70.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "UbSGy8s7HGne"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT-t_UjpIdjv",
    "outputId": "e92f7058-ad1b-4d96-d945-8a6ac6758248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.1932090207090207, subjectivity=0.44067421317421324)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9875, 'neg': 0.019, 'neu': 0.872, 'pos': 0.11}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_71.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "fTBfTipBHGql"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yD-CwSFCIeoF",
    "outputId": "74775031-197d-4071-a13d-d428685cc091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.15952607278652906, subjectivity=0.4624773673728044)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9998, 'neg': 0.01, 'neu': 0.823, 'pos': 0.168}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_72.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "bgZWNqa_HGtq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wrTLvGIf1K",
    "outputId": "78962d23-7aaf-4dab-bdaa-cd9c3089dd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07914249247129679, subjectivity=0.4077649162431774)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9994, 'neg': 0.057, 'neu': 0.829, 'pos': 0.113}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_73.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "9sppdADTHGwt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlh2Bop9IhAZ",
    "outputId": "a786ef07-13d2-4aff-f40f-86fa05e9bd38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.1314492794362924, subjectivity=0.414484764874375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 1.0, 'neg': 0.022, 'neu': 0.792, 'pos': 0.186}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_74.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ddDXNMXnHGzz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJ2znxK8IiLO",
    "outputId": "3734d06b-0437-406c-ea11-c49af58d48a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07009002361275089, subjectivity=0.4204095904095905)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9982, 'neg': 0.059, 'neu': 0.83, 'pos': 0.111}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_75.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "Gej9aFS_HG3E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U80Wqz2BIjQH",
    "outputId": "0f085552-6be2-4af0-dd93-b20b0dd2377b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.02282190132370638, subjectivity=0.39419718067732523)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9383, 'neg': 0.083, 'neu': 0.844, 'pos': 0.073}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_76.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "qMory5l0HG8e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eN9p4FMmIkr3",
    "outputId": "be8d80ac-01f1-4845-ef93-42f3a95997fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07131270226537217, subjectivity=0.4438904299583911)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.996, 'neg': 0.15, 'neu': 0.725, 'pos': 0.125}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_77.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "yQf_eDZ0HHB2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3wA-ZpqImNb",
    "outputId": "16a4fd30-ab14-46b0-e1f6-e645a0ff3e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10558024745524747, subjectivity=0.4331493506493504)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9808, 'neg': 0.132, 'neu': 0.707, 'pos': 0.161}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_78.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "s4cjJuSdHHEy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfGifUtBInhd",
    "outputId": "2e6344f0-b299-400d-ffd7-64ded0db005e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06905391120507398, subjectivity=0.4072561159770463)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9989, 'neg': 0.06, 'neu': 0.814, 'pos': 0.125}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_79.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "c8ZcvKonHHHi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW43q4y7IoyX",
    "outputId": "f8526b5e-3d25-470f-e4c5-08890e961fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08432727081114662, subjectivity=0.316335721431568)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9899, 'neg': 0.088, 'neu': 0.839, 'pos': 0.073}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_80.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "-wQl9oMLHHKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_esxk7EIqCd",
    "outputId": "46060345-56e1-4890-9a56-fc333a3bccff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.03849517079559095, subjectivity=0.4168776601549712)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9964, 'neg': 0.145, 'neu': 0.731, 'pos': 0.124}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_81.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "32d2es12HHNx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vNqsI2sIrMX",
    "outputId": "37de500a-b136-46d8-ef07-913f8fb7cf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05927489177489177, subjectivity=0.3835103230103231)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9676, 'neg': 0.097, 'neu': 0.827, 'pos': 0.076}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_82.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "xvCnfovHHHQp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riREdTQcItUc",
    "outputId": "2bb76a0f-5bf8-4d68-85a1-da97628a23fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05927489177489177, subjectivity=0.3835103230103231)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9676, 'neg': 0.097, 'neu': 0.827, 'pos': 0.076}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_83.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "4A59WtGUHHWT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "931HyRb5Iunf",
    "outputId": "7c380523-0c5a-4769-f6bd-3dab348bef34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.048182812557812545, subjectivity=0.37137267053933726)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9995, 'neg': 0.174, 'neu': 0.73, 'pos': 0.096}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_84.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "IxU9-0AlHHYk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aZK-VsIIv1z",
    "outputId": "79ea72cb-9987-4f6b-dd77-a3cf1cb81d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06863275613275616, subjectivity=0.4415515040515044)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9678, 'neg': 0.122, 'neu': 0.763, 'pos': 0.115}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_85.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word) \n",
    "         \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "ADisDK_VHHbO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmdCZK-uIw_d",
    "outputId": "7b4965a6-41ec-46fc-8c42-9deccdf3e94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10522180451127819, subjectivity=0.4173909774436091)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9977, 'neg': 0.027, 'neu': 0.844, 'pos': 0.129}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_86.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "X5VkACRDHHfa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1tmnIluIyPY",
    "outputId": "e68d57e2-30fb-435a-b8df-12d2a9854b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.0912718932443703, subjectivity=0.34346538782318586)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9995, 'neg': 0.037, 'neu': 0.78, 'pos': 0.183}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_87.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "6toLbuZCHHiO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnwGWTfcIzdb",
    "outputId": "0349505a-1796-4b84-ef77-8b021909f812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06315112160566706, subjectivity=0.4008890200708381)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9983, 'neg': 0.151, 'neu': 0.728, 'pos': 0.121}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_88.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "WqHY4-McHHqC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOjSLNj1I09g",
    "outputId": "e81e8928-c8e4-4130-eb5f-ae667e2648d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.2378919860627177, subjectivity=0.5332752613240419)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9937, 'neg': 0.116, 'neu': 0.73, 'pos': 0.154}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_89.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "cg5UDyToHHsu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9gXMBqJI2I7",
    "outputId": "07751025-9ae5-4408-cca7-74db37648320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.01614723743870664, subjectivity=0.2905879034078088)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9995, 'neg': 0.134, 'neu': 0.812, 'pos': 0.054}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_90.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "qVSXPFdcHHvk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3LGf7_HI3JG",
    "outputId": "2609ab98-ff16-4954-8ca3-05d1cf1e3fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06958410636982064, subjectivity=0.4712213976499689)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9987, 'neg': 0.129, 'neu': 0.799, 'pos': 0.072}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_91.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "        \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "M7oPaSr5HHyn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsJjG2feI4QR",
    "outputId": "d2a18d3b-7bed-44ae-d9d7-073c8ed02187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.0998917748917749, subjectivity=0.4394679881521986)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9453, 'neg': 0.11, 'neu': 0.777, 'pos': 0.112}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_92.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)  \n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "RSMV_tW4HH1u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cr6_SIixI5OQ",
    "outputId": "d50359d8-542d-41ef-d7b0-8ad44e48c80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.05508002700663251, subjectivity=0.38465877649363883)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9953, 'neg': 0.101, 'neu': 0.781, 'pos': 0.119}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_93.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHpVLO2XwRHJ",
    "outputId": "5b093f42-de4f-45b5-97ab-a2c06eb19877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.006907199942914224, subjectivity=0.4886575032110748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.997, 'neg': 0.1, 'neu': 0.769, 'pos': 0.131}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_94.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wjq9BBjswSJy",
    "outputId": "f1dee8da-5093-462d-82e5-c4a951ef0625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.019398926237161523, subjectivity=0.3703151260504203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9986, 'neg': 0.07, 'neu': 0.829, 'pos': 0.102}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_95.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dm0hVeTgwSPl",
    "outputId": "eaa87640-e65c-4e2f-ca5a-71f2966c112c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.047425426136363615, subjectivity=0.46793408414502163)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.1547, 'neg': 0.129, 'neu': 0.75, 'pos': 0.121}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_96.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-VJ9I9YwSVC",
    "outputId": "142714dd-703e-49b6-eac8-6c3174ee1cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.058711749304532865, subjectivity=0.4141077565046638)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9973, 'neg': 0.111, 'neu': 0.795, 'pos': 0.094}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_97.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKVNxzJKwScV",
    "outputId": "72a55e1d-29e7-4585-876d-412e18c7a4af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11662789475896264, subjectivity=0.4963353757892592)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9993, 'neg': 0.115, 'neu': 0.718, 'pos': 0.167}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_98.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EF9KZTWxwSiJ",
    "outputId": "60ae4139-c2a2-44fa-c923-dd63676b7b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.04979603729603729, subjectivity=0.45818764568764575)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9848, 'neg': 0.109, 'neu': 0.808, 'pos': 0.083}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_99.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myUvcbkvwSn2",
    "outputId": "30c50cae-350c-4ce1-d6e8-9e4ff45e0b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.034325222969801276, subjectivity=0.3153324988264746)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9999, 'neg': 0.158, 'neu': 0.745, 'pos': 0.097}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_100.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNGpZLVRwSte",
    "outputId": "d5f181a4-ec5f-4a4a-d33e-7f39a4169b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09471800454738141, subjectivity=0.4840979742315059)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9995, 'neg': 0.119, 'neu': 0.695, 'pos': 0.186}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_101.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3TWbl4wTJN",
    "outputId": "607c8740-78d5-472c-c0d4-8787f2b781f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.07041002811621364, subjectivity=0.4225609184629802)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9974, 'neg': 0.11, 'neu': 0.8, 'pos': 0.089}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_102.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0l52FbfIwTSJ",
    "outputId": "ba06ba36-c4c9-4de4-bb34-2baee0059a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09145088502231363, subjectivity=0.42909210159210154)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.992, 'neg': 0.102, 'neu': 0.787, 'pos': 0.111}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_103.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpakbU7BwTa9",
    "outputId": "44832a39-e55a-4384-c745-04dcd905a77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.06975772118177179, subjectivity=0.3636222427361669)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.7551, 'neg': 0.106, 'neu': 0.785, 'pos': 0.108}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_104.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_ECUF6NwTgR",
    "outputId": "b7686fbc-3dc3-4672-d305-41b8f2ec23d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.13318385270082406, subjectivity=0.35316735049111003)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.036, 'neu': 0.853, 'pos': 0.111}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_105.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ltau0n7nwTln",
    "outputId": "157cc4bc-42bf-4b9e-b39a-39cb7c0a5765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.040846281694495994, subjectivity=0.3698022959183672)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9976, 'neg': 0.11, 'neu': 0.819, 'pos': 0.071}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_106.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qi8gw8h-wT1y",
    "outputId": "f02a0e7c-867d-4b90-f1e8-fdd9122606cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.009649198662515839, subjectivity=0.4414072408624464)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9998, 'neg': 0.19, 'neu': 0.706, 'pos': 0.104}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_107.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZB_SvuzwwUHS",
    "outputId": "16aec5b6-1d88-499b-e148-2328e2d809fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.18930555555555537, subjectivity=0.5275595238095244)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9996, 'neg': 0.09, 'neu': 0.726, 'pos': 0.184}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_108.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oqhWxxoxXmV",
    "outputId": "6a1b262e-7933-48ce-bc92-68da5ced0214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.020506305288913992, subjectivity=0.4016292584227367)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9931, 'neg': 0.062, 'neu': 0.902, 'pos': 0.036}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_109.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToUoWSFbxX04",
    "outputId": "5bec7280-fa69-49e2-bc2f-185e57e95e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.012604529616724742, subjectivity=0.37270905923344966)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9953, 'neg': 0.097, 'neu': 0.837, 'pos': 0.066}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_110.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49JBNCocxYCj",
    "outputId": "d712ee1c-9c17-4ea7-9aa0-0a1e7da342b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.11454773532252849, subjectivity=0.44620519115870366)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 1.0, 'neg': 0.041, 'neu': 0.766, 'pos': 0.193}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_111.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzHIwcSuxYlY",
    "outputId": "deef0d90-33ec-4645-ebab-19ded452a625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08994611059044053, subjectivity=0.44009305127861836)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9967, 'neg': 0.046, 'neu': 0.875, 'pos': 0.079}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_112.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvORC6ecxYrS",
    "outputId": "f090092b-9cd5-4d96-e4ea-393953072c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.04588320389827926, subjectivity=0.42106517435663166)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9997, 'neg': 0.093, 'neu': 0.714, 'pos': 0.192}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_113.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnwDjRkRxYzG",
    "outputId": "3910ba4b-673c-4239-d7d4-0a4eecf2897c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.04924299990789351, subjectivity=0.31249424334530723)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.6346, 'neg': 0.116, 'neu': 0.758, 'pos': 0.126}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_114.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_LEPaKSxY49",
    "outputId": "57f0ac08-c915-4819-e287-5c17cddc6c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.13706293706293707, subjectivity=0.37559440559440577)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9224, 'neg': 0.083, 'neu': 0.806, 'pos': 0.111}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_115.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7biPlASixZAB",
    "outputId": "13990c94-b4c5-405b-fc49-231f0a35ad74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.10892857142857142, subjectivity=0.42964285714285716)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9871, 'neg': 0.126, 'neu': 0.803, 'pos': 0.071}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_116.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtbvJp1JxZFy",
    "outputId": "5b8e03fe-955a-43c7-adb3-b5b3ad89d45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.0015771370023419243, subjectivity=0.30157015116031516)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9915, 'neg': 0.105, 'neu': 0.86, 'pos': 0.035}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_117.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqDzTE88xZLp",
    "outputId": "07bd5b69-60fe-4de8-d8a6-00828f601935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.055508405553531884, subjectivity=0.42907687979890163)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9998, 'neg': 0.183, 'neu': 0.745, 'pos': 0.072}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_118.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6M2_OUxlxZRP",
    "outputId": "8788a158-2c31-442a-bffa-ab0e9ce8ec01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.14342926304464768, subjectivity=0.4502981633750864)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9999, 'neg': 0.054, 'neu': 0.695, 'pos': 0.251}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_119.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mIcvykHxZZa",
    "outputId": "082729b3-2548-4dd3-d828-0f6c134ecff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.055508405553531884, subjectivity=0.42907687979890163)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9998, 'neg': 0.183, 'neu': 0.745, 'pos': 0.072}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_120.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flycZs-wxZjx",
    "outputId": "ad0e4ab9-dd09-4f83-b9f3-8aa9791fa6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=-0.02338902325744431, subjectivity=0.41091366966366966)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': -0.9982, 'neg': 0.126, 'neu': 0.794, 'pos': 0.08}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_121.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpqNvazWxZrk",
    "outputId": "fed1e492-19a2-44a8-c364-697f1ffd803c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09210009592680049, subjectivity=0.38031040550358736)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9955, 'neg': 0.085, 'neu': 0.813, 'pos': 0.102}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_122.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVpktYQ2xZxa",
    "outputId": "27f19ef2-1df0-43f2-c378-e4ae01148716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.10030268118977796, subjectivity=0.3450795978215333)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.7455, 'neg': 0.129, 'neu': 0.739, 'pos': 0.132}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_123.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7QEFne0xZ3E",
    "outputId": "31925c82-63e4-4dbe-a4a9-0c1bd6e5042e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.08852647352647353, subjectivity=0.48836663336663344)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9994, 'neg': 0.046, 'neu': 0.82, 'pos': 0.134}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_124.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdQaij4oxZ9M",
    "outputId": "c4705640-b6fe-40c5-c3be-33ed202be835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Sentiment(polarity=0.09374697336561744, subjectivity=0.4423481638418077)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9992, 'neg': 0.041, 'neu': 0.843, 'pos': 0.116}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data. \n",
    "with open('URL_id_125.txt','r', encoding='latin1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#text normalization\n",
    "\n",
    "#spliting at ( \\n ).\n",
    "text = text.split('\\n')\n",
    "#seperating at new line using '\\n'\n",
    "\n",
    "#spliting at (\\t)\n",
    "corpus = [text]\n",
    "for sent in text:\n",
    "  corpus.append(sent.split('\\t'))\n",
    "  #splitting string by tab(\\t)\n",
    "#taking only letters\n",
    "letters_only = re.sub(r'[^a-zA-Z]', \n",
    "                            \" \",\n",
    "                            str(corpus))\n",
    "#tokenization\n",
    "#coverting to lower case\n",
    "letters_only = letters_only.lower()\n",
    "token = nltk.sent_tokenize(letters_only)\n",
    "\n",
    "#alphanumeric characters\n",
    "def num_dec_al(word):\n",
    "    if word.isnumeric():\n",
    "        return 'xxxxxx'\n",
    "    elif word.isdecimal():\n",
    "        return 'xxx...'\n",
    "    elif word.isalpha():\n",
    "        return word\n",
    "    else:\n",
    "        return 'xxxaaa'\n",
    "\n",
    "def clean_nda(token):\n",
    "    tokens = nltk.word_tokenize(token)\n",
    "    map_list = list(map(num_dec_al,tokens))\n",
    "    return \" \".join(map_list)\n",
    "\n",
    "corpus_nda = list(map(clean_nda,token))\n",
    "## Alpha numeric characters and decimal have been replaced with characters \n",
    "\n",
    "#removing contractions\n",
    "conm = contractions.contractions_dict\n",
    "def contraction_remove(corpus_nda):\n",
    "    for key,value in conm.items():\n",
    "        corpus_nda = re.sub(r\"{}\".format(key),'{}'.format(value),corpus_nda)\n",
    "        \n",
    "    return corpus_nda\n",
    "\n",
    "special = string.punctuation\n",
    "def w_tokenization(corpus_nda):\n",
    "    # convert into lower case\n",
    "    corpus_nda = corpus_nda.lower()\n",
    "    # contraction\n",
    "    corpus_nda = contraction_remove(corpus_nda)\n",
    "    # \n",
    "    tokens = nltk.word_tokenize(corpus_nda) # word tokens\n",
    "## contractions have been expanded and the letters have been converted to lowercase.   \n",
    "\n",
    "#seperating at tab\n",
    "data = [corpus_nda]\n",
    "for sent in text:\n",
    "  data.append(sent.split('\\t'))\n",
    "#separating at new line\n",
    "data.append(sent.split('\\n'))\n",
    "\n",
    "#lemmatization of data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def __init__(self):\n",
    "  pass\n",
    "def lemmatize(self, word):\n",
    "  lemmas = wordnet.morphy(words, pos)\n",
    "  return min (lemmas,key=len) if lemmas else word\n",
    "def __repr__(self):\n",
    "  return 'WordNetLemmatizer'\n",
    "\n",
    "  #unload wordnet\n",
    "def teardown_module(module=None):\n",
    "  from nltk.corpus import wordnet\n",
    "  wordnet._unload()\n",
    "\n",
    "#saving stopWords in stop\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#removing stopwords from text and printing the words without stopwords\n",
    "text = data\n",
    "text_tokens = word_tokenize(str(text))\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "tokens_without_sw=(str(tokens_without_sw))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentence = tokens_without_sw\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:        \n",
    "        neu_word_list.append(word)\n",
    "          \n",
    "blob = TextBlob(sentence)\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment) \n",
    "\n",
    "score = sid.polarity_scores(sentence)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentimental_analysis_ for_URL_id_1-125.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
